name: Auto-update dispersion CSVs

on:
  # run on a schedule (example: every day at 08:00 UTC)
  schedule:
    - cron: "0 8 * * *"
  # allow manual run from GitHub UI
  workflow_dispatch:

permissions:
  contents: write  # needed so the workflow can push commits

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout full history so we can rebase/pull
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 4. Expose POLYGON_API_KEY to the script
      - name: Add POLYGON_API_KEY
        run: echo "POLYGON_API_KEY=${{ secrets.POLYGON_API_KEY }}" >> $GITHUB_ENV

      # 5. Run your data update script
      - name: Run update.py
        run: python update.py

      # 6. Commit and push updated CSVs
      - name: Commit and push updated CSVs
        if: success()
        run: |
          git config --local user.email "actions@github.com"
          git config --local user.name "GitHub Actions"

          # Make sure this clone is up to date with remote main to avoid
          # non-fast-forward push errors.
          git fetch origin main
          git pull --rebase origin main

          # Stage your CSVs (change path if needed)
          git add data/*.csv

          # If nothing changed, stop here
          if git diff --cached --quiet; then
            echo "No CSV changes to commit."
            exit 0
          fi

          git commit -m "Auto-update dispersion CSVs [skip ci]"
          git push origin HEAD:main
