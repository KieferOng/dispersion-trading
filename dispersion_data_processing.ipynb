{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819a8a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 files\n",
      "Detected 13 tickers: ['AAPL', 'AMZN', 'AVGO', 'GOOG', 'JPM', 'LLY', 'META', 'MSFT', 'NVDA', 'SPY', 'TSLA', 'V', 'WMT']\n",
      "[AAPL] ✅ merged 11 files → 1363 rows → data2\\AAPL_options_daily_merged.csv\n",
      "[AMZN] ✅ merged 8 files → 987 rows → data2\\AMZN_options_daily_merged.csv\n",
      "[AVGO] ✅ merged 3 files → 371 rows → data2\\AVGO_options_daily_merged.csv\n",
      "[GOOG] ✅ merged 7 files → 864 rows → data2\\GOOG_options_daily_merged.csv\n",
      "[JPM] ✅ merged 11 files → 1363 rows → data2\\JPM_options_daily_merged.csv\n",
      "[LLY] ✅ merged 11 files → 1363 rows → data2\\LLY_options_daily_merged.csv\n",
      "[META] ✅ merged 7 files → 850 rows → data2\\META_options_daily_merged.csv\n",
      "[MSFT] ✅ merged 11 files → 1362 rows → data2\\MSFT_options_daily_merged.csv\n",
      "[NVDA] ✅ merged 3 files → 371 rows → data2\\NVDA_options_daily_merged.csv\n",
      "[SPY] ✅ merged 11 files → 1351 rows → data2\\SPY_options_daily_merged.csv\n",
      "[TSLA] ✅ merged 7 files → 862 rows → data2\\TSLA_options_daily_merged.csv\n",
      "[V] ✅ merged 11 files → 1363 rows → data2\\V_options_daily_merged.csv\n",
      "[WMT] ✅ merged 4 files → 493 rows → data2\\WMT_options_daily_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "INPUT_DIR = \"data\"\n",
    "OUTPUT_DIR = \"data2\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# pattern: TICKER_options_daily_YYYY-MM-DD_YYYY-MM-DD.csv\n",
    "pattern = os.path.join(INPUT_DIR, \"*_options_daily_*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "# group by ticker\n",
    "by_ticker = defaultdict(list)\n",
    "for f in files:\n",
    "    base = os.path.basename(f)\n",
    "    if \"_options_daily_\" not in base:\n",
    "        continue\n",
    "    ticker = base.split(\"_options_daily_\")[0]\n",
    "    by_ticker[ticker].append(f)\n",
    "\n",
    "print(f\"Detected {len(by_ticker)} tickers:\", sorted(by_ticker.keys()))\n",
    "\n",
    "for ticker, flist in sorted(by_ticker.items()):\n",
    "    flist = sorted(flist)\n",
    "    dfs = []\n",
    "\n",
    "    for path in flist:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            print(f\"[{ticker}] ❌ failed to read {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"date_ny\" not in df.columns:\n",
    "            print(f\"[{ticker}] ⚠️ {path} missing 'date_ny', skipping\")\n",
    "            continue\n",
    "\n",
    "        df[\"date_ny\"] = pd.to_datetime(df[\"date_ny\"], errors=\"coerce\").dt.date\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"[{ticker}] ⚠️ no valid chunks, skipping\")\n",
    "        continue\n",
    "\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "    merged = merged[merged[\"date_ny\"].notna()]\n",
    "\n",
    "    merged = merged.sort_values(\"date_ny\")\n",
    "    merged = merged.drop_duplicates(subset=[\"date_ny\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{ticker}_options_daily_merged.csv\")\n",
    "    merged.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"[{ticker}] ✅ merged {len(flist)} files → {len(merged)} rows → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f33b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 merged files to clean\n",
      "[AAPL] ✅ cleaned → 1298 rows ≥ 2020-09-01\n",
      "[AMZN] ✅ cleaned → 854 rows ≥ 2022-06-07\n",
      "[AVGO] ✅ cleaned → 326 rows ≥ 2024-07-16\n",
      "[GOOG] ✅ cleaned → 826 rows ≥ 2022-07-19\n",
      "[JPM] ✅ cleaned → 1363 rows ≥ 2020-05-31\n",
      "[LLY] ✅ cleaned → 1363 rows ≥ 2020-05-31\n",
      "[META] ✅ cleaned → 849 rows ≥ 2022-06-11\n",
      "[MSFT] ✅ cleaned → 1362 rows ≥ 2020-05-31\n",
      "[NVDA] ✅ cleaned → 348 rows ≥ 2024-06-12\n",
      "[SPY] ✅ cleaned → 1351 rows ≥ 2020-05-31\n",
      "[TSLA] ✅ cleaned → 796 rows ≥ 2022-08-26\n",
      "[V] ✅ cleaned → 1363 rows ≥ 2020-05-31\n",
      "[WMT] ✅ cleaned → 422 rows ≥ 2024-02-27\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "MERGED_DIR = \"data2\"\n",
    "\n",
    "CUTOFFS = {\n",
    "    \"SPY\":   \"2020-05-31\",\n",
    "    \"AAPL\":  \"2020-09-01\",\n",
    "    \"MSFT\":  \"2020-05-31\",\n",
    "    \"NVDA\":  \"2024-06-12\",\n",
    "    \"AMZN\":  \"2022-06-07\",\n",
    "    \"META\":  \"2022-06-11\",\n",
    "    \"AVGO\":  \"2024-07-16\",\n",
    "    \"TSLA\":  \"2022-08-26\",\n",
    "    \"WMT\":   \"2024-02-27\",\n",
    "    \"JPM\":   \"2020-05-31\",\n",
    "    \"GOOG\":  \"2022-07-19\",\n",
    "    \"V\":     \"2020-05-31\",\n",
    "    \"LLY\":   \"2020-05-31\"\n",
    "}\n",
    "\n",
    "DEFAULT_CUTOFF = \"2020-05-31\"\n",
    "KEY_COLS = [\n",
    "    \"S_close\",\n",
    "    \"total_vol\",\n",
    "    \"avg_iv\",\n",
    "    \"avg_vega\",\n",
    "    \"call_vol\",\n",
    "    \"put_vol\",\n",
    "    \"call_iv\",\n",
    "    \"put_iv\",\n",
    "]\n",
    "\n",
    "files = [f for f in os.listdir(MERGED_DIR) if f.endswith(\"_options_daily_merged.csv\")]\n",
    "print(f\"Found {len(files)} merged files to clean\")\n",
    "\n",
    "for fname in sorted(files):\n",
    "    path = os.path.join(MERGED_DIR, fname)\n",
    "    ticker = fname.split(\"_options_daily_merged.csv\")[0]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] ❌ failed to read merged file: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"date_ny\" not in df.columns:\n",
    "        print(f\"[{ticker}] ⚠️ no date_ny in merged file, skipping\")\n",
    "        continue\n",
    "\n",
    "    df[\"date_ny\"] = pd.to_datetime(df[\"date_ny\"], errors=\"coerce\").dt.date\n",
    "    df = df[df[\"date_ny\"].notna()]\n",
    "\n",
    "    cutoff_str = CUTOFFS.get(ticker, DEFAULT_CUTOFF)\n",
    "    cutoff = pd.to_datetime(cutoff_str).date()\n",
    "    df = df[df[\"date_ny\"] >= cutoff]\n",
    "\n",
    "    existing_keys = [c for c in KEY_COLS if c in df.columns]\n",
    "    if existing_keys:\n",
    "        df = df.dropna(subset=existing_keys, how=\"all\")\n",
    "\n",
    "    df = df.sort_values(\"date_ny\").drop_duplicates(subset=[\"date_ny\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"[{ticker}] ⚠️ all rows removed after cleaning; leaving file unchanged\")\n",
    "        continue\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"[{ticker}] ✅ cleaned → {len(df)} rows ≥ {cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a208b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 ticker files.\n",
      "Loaded 13 tickers: ['AAPL', 'AMZN', 'AVGO', 'GOOG', 'JPM', 'LLY', 'META', 'MSFT', 'NVDA', 'SPY', 'TSLA', 'V', 'WMT']\n",
      "Merge order: ['SPY', 'JPM', 'LLY', 'MSFT', 'V', 'AAPL', 'AMZN', 'META', 'GOOG', 'TSLA', 'WMT', 'NVDA', 'AVGO']\n",
      "✅ Master dataset saved → data2\\master_dispersion_data.csv\n",
      "Rows: 1363, Columns: 53\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIG ---\n",
    "INPUT_FOLDER = \"data2\"\n",
    "OUTPUT_FILE = os.path.join(INPUT_FOLDER, \"master_dispersion_data.csv\")\n",
    "\n",
    "# --- Collect all merged ticker CSVs ---\n",
    "csv_files = [\n",
    "    f for f in os.listdir(INPUT_FOLDER)\n",
    "    if f.endswith(\"_options_daily_merged.csv\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(csv_files)} ticker files.\")\n",
    "\n",
    "ticker_dfs = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    ticker = file.split(\"_options_daily_merged\")[0]\n",
    "    path = os.path.join(INPUT_FOLDER, file)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if \"date_ny\" not in df.columns:\n",
    "        print(f\"[{ticker}] ⚠️ missing date_ny column, skipping\")\n",
    "        continue\n",
    "\n",
    "    df[\"date_ny\"] = pd.to_datetime(df[\"date_ny\"], errors=\"coerce\")\n",
    "    df = df[df[\"date_ny\"].notna()]\n",
    "\n",
    "    required = [\"total_vol\", \"avg_vega\", \"avg_iv\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[{ticker}] ⚠️ missing columns {missing}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # compute Vs here before merge\n",
    "    df[\"Vs\"] = df[\"total_vol\"] * df[\"avg_vega\"]\n",
    "\n",
    "    # clean + sort\n",
    "    df = df.sort_values(\"date_ny\").drop_duplicates(\"date_ny\", keep=\"last\").reset_index(drop=True)\n",
    "    ticker_dfs[ticker] = df\n",
    "\n",
    "print(f\"Loaded {len(ticker_dfs)} tickers: {sorted(ticker_dfs.keys())}\")\n",
    "\n",
    "# --- Build full date range ---\n",
    "full_range = pd.date_range(start=\"2020-06-01\", end=\"2025-10-30\", freq=\"D\")\n",
    "master = pd.DataFrame({\"date_ny\": full_range})\n",
    "\n",
    "# --- Order tickers: SPY first, then others by earliest start ---\n",
    "start_dates = {t: ticker_dfs[t][\"date_ny\"].min() for t in ticker_dfs}\n",
    "ordered_tickers = [\"SPY\"] + sorted(\n",
    "    [t for t in ticker_dfs.keys() if t != \"SPY\"],\n",
    "    key=lambda x: start_dates[x]\n",
    ")\n",
    "\n",
    "print(\"Merge order:\", ordered_tickers)\n",
    "\n",
    "# --- Merge each ticker into master ---\n",
    "for ticker in ordered_tickers:\n",
    "    df = ticker_dfs[ticker][[\"date_ny\", \"total_vol\", \"avg_vega\", \"Vs\", \"avg_iv\"]].copy()\n",
    "    df = df.rename(columns={\n",
    "        \"total_vol\": f\"{ticker}_vol\",\n",
    "        \"avg_vega\": f\"{ticker}_avgVega\",\n",
    "        \"Vs\": f\"{ticker}_Vs\",\n",
    "        \"avg_iv\": f\"{ticker}_IV\",\n",
    "    })\n",
    "    master = master.merge(df, on=\"date_ny\", how=\"left\")\n",
    "\n",
    "# --- Drop rows where all ticker columns are NaN ---\n",
    "ticker_cols = [col for col in master.columns if col != \"date_ny\"]\n",
    "master = master.dropna(subset=ticker_cols, how=\"all\")\n",
    "\n",
    "# --- Save output ---\n",
    "master.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Master dataset saved → {OUTPUT_FILE}\")\n",
    "print(f\"Rows: {len(master)}, Columns: {len(master.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b73059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated master_dispersion_data.csv with Singles_* fields and clean DD/MM/YYYY dates.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_FOLDER = \"data2\"\n",
    "MASTER_FILE = os.path.join(INPUT_FOLDER, \"master_dispersion_data.csv\")\n",
    "\n",
    "# --- load master ---\n",
    "df = pd.read_csv(MASTER_FILE)\n",
    "\n",
    "# parse dates as day-first with /, allowing 1- or 2-digit day/month\n",
    "# format=\"mixed\" lets pandas handle mixed \"6/1/2020\", \"06/01/2020\", etc.\n",
    "df[\"date_ny\"] = pd.to_datetime(\n",
    "    df[\"date_ny\"],\n",
    "    format=\"mixed\",\n",
    "    dayfirst=True,\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "if df[\"date_ny\"].isna().sum() > 0:\n",
    "    print(\"⚠️ Some date_ny entries could not be parsed. Check those rows manually.\")\n",
    "\n",
    "# --- identify columns ---\n",
    "vs_cols = [c for c in df.columns if c.endswith(\"_Vs\")]\n",
    "iv_cols = [c for c in df.columns if c.endswith(\"_IV\")]\n",
    "\n",
    "# single-stock = everything except SPY\n",
    "single_vs_cols = [c for c in vs_cols if not c.startswith(\"SPY_\")]\n",
    "single_iv_cols = [c for c in iv_cols if not c.startswith(\"SPY_\")]\n",
    "\n",
    "# --- aggregates (excluding SPY) ---\n",
    "# 1) how many single stocks have data (Vs not NaN)\n",
    "df[\"Singles_count\"] = df[single_vs_cols].notna().sum(axis=1)\n",
    "\n",
    "# 2) sum of Vs across single stocks\n",
    "df[\"Singles_Vs_sum\"] = df[single_vs_cols].sum(axis=1, skipna=True)\n",
    "\n",
    "# 3) average Vs across single stocks with data\n",
    "df[\"Singles_Vs_avg\"] = df[single_vs_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# 4) average IV across single stocks with data\n",
    "df[\"Singles_IV_avg\"] = df[single_iv_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# --- reorder columns: date + 4 aggregates first ---\n",
    "front_cols = [\"date_ny\", \"Singles_count\", \"Singles_Vs_sum\", \"Singles_Vs_avg\", \"Singles_IV_avg\"]\n",
    "other_cols = [c for c in df.columns if c not in front_cols]\n",
    "df = df[front_cols + other_cols]\n",
    "\n",
    "# --- write dates back as DD/MM/YYYY with \"/\" ---\n",
    "df[\"date_ny\"] = df[\"date_ny\"].dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "df.to_csv(MASTER_FILE, index=False)\n",
    "\n",
    "print(\"✅ Updated master_dispersion_data.csv with Singles_* fields and clean DD/MM/YYYY dates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea020e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dispersion factors saved to: data2\\dispersion_factors.csv\n",
      "Total rows after dropping SPY-missing: 1348\n",
      "     date_ny        F1        F2        F3\n",
      "0 2020-06-01  0.162529  0.105292  0.712246\n",
      "1 2020-06-02  0.222298  0.114830  0.785626\n",
      "2 2020-06-03  0.108393  0.121592  0.795996\n",
      "3 2020-06-04  0.056167  0.103976  0.726426\n",
      "4 2020-06-05  0.180939  0.133809  0.986852\n",
      "        date_ny        F1        F2        F3\n",
      "1343 2025-10-24  0.369670  0.363109  1.062936\n",
      "1344 2025-10-27  0.648049  0.479351  1.073749\n",
      "1345 2025-10-28  0.761703  0.489011  1.035718\n",
      "1346 2025-10-29  0.720832  0.601174  1.049872\n",
      "1347 2025-10-30  0.473387  0.395753  1.026657\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "INPUT_FOLDER = \"data2\"\n",
    "OUTPUT_FOLDER = \"data2\"\n",
    "\n",
    "MASTER_FILE = os.path.join(INPUT_FOLDER, \"master_dispersion_data.csv\")\n",
    "OUT_FILE = os.path.join(OUTPUT_FOLDER, \"dispersion_factors.csv\")\n",
    "\n",
    "# --- Load master ---\n",
    "df = pd.read_csv(MASTER_FILE)\n",
    "df[\"date_ny\"] = pd.to_datetime(df[\"date_ny\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# --- Drop rows with missing SPY data (IV or Vs) ---\n",
    "df = df.dropna(subset=[\"SPY_IV\", \"SPY_Vs\"]).reset_index(drop=True)\n",
    "\n",
    "# --- Identify single-stock IV columns (exclude SPY) ---\n",
    "stock_iv_cols = [c for c in df.columns if c.endswith(\"_IV\") and not c.startswith(\"SPY_\")]\n",
    "\n",
    "# --- F1: vega-exposure share (singles vs total) ---\n",
    "df[\"F1\"] = df[\"Singles_Vs_sum\"] / (df[\"Singles_Vs_sum\"] + df[\"SPY_Vs\"])\n",
    "\n",
    "# --- F2: IV spread (singles minus SPY) ---\n",
    "df[\"F2\"] = df[\"Singles_IV_avg\"] - df[\"SPY_IV\"]\n",
    "\n",
    "# --- F3: 1 - implied correlation (constant-corr, equal-weight) ---\n",
    "F3 = []\n",
    "for _, row in df.iterrows():\n",
    "    sigma_index = row[\"SPY_IV\"]\n",
    "    sigmas = row[stock_iv_cols].dropna().values\n",
    "    N = len(sigmas)\n",
    "    if N < 2:\n",
    "        F3.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    sum_sigma = np.sum(sigmas)\n",
    "    sum_sigma_sq = np.sum(sigmas ** 2)\n",
    "    numerator = (N ** 2) * (sigma_index ** 2) - sum_sigma_sq\n",
    "    denominator = (sum_sigma ** 2) - sum_sigma_sq\n",
    "    if denominator <= 0:\n",
    "        F3.append(np.nan)\n",
    "        continue\n",
    "    rho_impl = numerator / denominator\n",
    "    if rho_impl < -0.5 or rho_impl > 1.5:\n",
    "        F3.append(np.nan)\n",
    "        continue\n",
    "    F3.append(1 - rho_impl)\n",
    "\n",
    "df[\"F3\"] = F3\n",
    "\n",
    "# --- Save cleaned factors ---\n",
    "df_out = df[[\"date_ny\", \"F1\", \"F2\", \"F3\"]].copy()\n",
    "df_out.to_csv(OUT_FILE, index=False)\n",
    "\n",
    "print(f\"✅ Dispersion factors saved to: {OUT_FILE}\")\n",
    "print(f\"Total rows after dropping SPY-missing: {len(df_out)}\")\n",
    "print(df_out.head())\n",
    "print(df_out.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efc6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Dispersion_Z_60d + EWMA(20,30,60) + SMA(60,90,120)\n",
      "         date_ny  Dispersion_Z_60d  Dispersion_Z_EWMA_20  \\\n",
      "1343  2025-10-24          1.582014             -0.083583   \n",
      "1344  2025-10-27          2.726209              0.184016   \n",
      "1345  2025-10-28          2.427981              0.397727   \n",
      "1346  2025-10-29          2.549316              0.602640   \n",
      "1347  2025-10-30          0.768905              0.618475   \n",
      "\n",
      "      Dispersion_Z_EWMA_30  Dispersion_Z_EWMA_60  Dispersion_Z_SMA_60  \\\n",
      "1343             -0.055029              0.037928             0.136448   \n",
      "1344              0.124406              0.126068             0.160770   \n",
      "1345              0.273023              0.201541             0.177765   \n",
      "1346              0.419881              0.278517             0.220929   \n",
      "1347              0.442399              0.294595             0.222259   \n",
      "\n",
      "      Dispersion_Z_SMA_90  Dispersion_Z_SMA_120  \n",
      "1343             0.165955              0.129075  \n",
      "1344             0.199259              0.152494  \n",
      "1345             0.243621              0.173204  \n",
      "1346             0.280596              0.200285  \n",
      "1347             0.290795              0.209526  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_FOLDER = \"data2\"\n",
    "FACTORS_FILE = os.path.join(INPUT_FOLDER, \"dispersion_factors.csv\")\n",
    "\n",
    "df = pd.read_csv(FACTORS_FILE)\n",
    "\n",
    "# --- ensure numeric for core factors ---\n",
    "for col in [\"F1\", \"F2\", \"F3\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# --- rolling 60d z-scores on F1, F2, F3 (use existing row order as time) ---\n",
    "window = 60\n",
    "min_pts = 40\n",
    "\n",
    "def rolling_z(series, win=60, min_pts=40):\n",
    "    m = series.rolling(win, min_periods=min_pts).mean()\n",
    "    s = series.rolling(win, min_periods=min_pts).std()\n",
    "    return (series - m) / s\n",
    "\n",
    "df[\"F1_z60\"] = rolling_z(df[\"F1\"], window, min_pts)\n",
    "df[\"F2_z60\"] = rolling_z(df[\"F2\"], window, min_pts)\n",
    "df[\"F3_z60\"] = rolling_z(df[\"F3\"], window, min_pts)\n",
    "\n",
    "# --- weighted composite of standardized factors ---\n",
    "w1, w2, w3 = 0.2, 0.4, 0.4\n",
    "df[\"Dispersion_Z_60d\"] = (\n",
    "    w1 * df[\"F1_z60\"] +\n",
    "    w2 * df[\"F2_z60\"] +\n",
    "    w3 * df[\"F3_z60\"]\n",
    ")\n",
    "\n",
    "# --- EWMA variants on composite (20, 30, 60) ---\n",
    "# Require at least `span` observations before emitting a value\n",
    "for span in [20, 30, 60]:\n",
    "    df[f\"Dispersion_Z_EWMA_{span}\"] = df[\"Dispersion_Z_60d\"].ewm(\n",
    "        span=span,\n",
    "        adjust=False,\n",
    "        min_periods=span\n",
    "    ).mean()\n",
    "\n",
    "# --- SMA variants on composite (60, 90, 120) ---\n",
    "# Only produce values once full lookback is available\n",
    "for win in [60, 90, 120]:\n",
    "    df[f\"Dispersion_Z_SMA_{win}\"] = df[\"Dispersion_Z_60d\"].rolling(\n",
    "        window=win,\n",
    "        min_periods=win\n",
    "    ).mean()\n",
    "\n",
    "# --- save back WITHOUT touching date_ny or order ---\n",
    "df.to_csv(FACTORS_FILE, index=False)\n",
    "\n",
    "print(\"✅ Saved Dispersion_Z_60d + EWMA(20,30,60) + SMA(60,90,120)\")\n",
    "print(df[[\n",
    "    \"date_ny\",\n",
    "    \"Dispersion_Z_60d\",\n",
    "    \"Dispersion_Z_EWMA_20\",\n",
    "    \"Dispersion_Z_EWMA_30\",\n",
    "    \"Dispersion_Z_EWMA_60\",\n",
    "    \"Dispersion_Z_SMA_60\",\n",
    "    \"Dispersion_Z_SMA_90\",\n",
    "    \"Dispersion_Z_SMA_120\"\n",
    "]].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b9adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
